import subprocess
import os
from pydub import AudioSegment

# --- CONFIGURATION: SET YOUR VOICES AND FILENAMES HERE ---

# The full dialogue script generated by LM Studio
SCRIPT_FILE = "script.txt"

# The final output filename for your podcast
FINAL_PODCAST_FILE = "final_podcast.mp3"

# The temporary file for each line's audio. This will be created and deleted.
TEMP_AUDIO_FILE = "_temp_line.wav"

# Assign which voice model to use for each speaker
VOICE_MAPPING = {
    "Host": "en_US-joe-medium.onnx",
    "Expert": "en_GB-southern_english_female-low.onnx" 
    # Add more speakers here if you want, e.g., "Analyst": "another_voice.onnx"
}

# The pause duration between each line of dialogue, in milliseconds
PAUSE_MS = 800  # 0.8 seconds

# --- END OF CONFIGURATION ---


def generate_audio_for_line(text, voice_model):
    """Calls Piper to generate a .wav file for a single line of text."""
    # Note: On Mac/Linux, the command might need to be "./piper" instead of "piper.exe"
    command = f'echo "{text}" | piper.exe --model {voice_model} --output_file {TEMP_AUDIO_FILE}'
    
    # Execute the command
    subprocess.run(command, shell=True, check=True, capture_output=True, text=True)


def main():
    """Main function to read the script and assemble the podcast."""
    print("Starting podcast assembly...")

    # Create an empty audio segment to build upon
    final_podcast = AudioSegment.silent(duration=1000) # Start with 1 second of silence

    try:
        with open(SCRIPT_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or ":" not in line:
                    continue # Skip empty lines or lines without a speaker

                # Identify the speaker and the dialogue text
                speaker, text = line.split(":", 1)
                speaker = speaker.strip()
                text = text.strip().replace('"', '\\"') # Escape quotes for the command line

                if speaker in VOICE_MAPPING:
                    print(f"Generating audio for {speaker}...")
                    
                    # 1. Generate the audio for the current line
                    voice_model = VOICE_MAPPING[speaker]
                    generate_audio_for_line(text, voice_model)

                    # 2. Load the generated audio clip
                    line_audio = AudioSegment.from_wav(TEMP_AUDIO_FILE)

                    # 3. Append it to the final podcast
                    final_podcast += line_audio

                    # 4. Add a pause after the line
                    final_podcast += AudioSegment.silent(duration=PAUSE_MS)
                else:
                    print(f"Warning: Speaker '{speaker}' not found in VOICE_MAPPING. Skipping line.")

        # Export the final combined audio file
        print(f"Exporting final podcast to {FINAL_PODCAST_FILE}...")
        final_podcast.export(FINAL_PODCAST_FILE, format="mp3")

        print("\nPodcast assembly complete!")

    finally:
        # Clean up the temporary file
        if os.path.exists(TEMP_AUDIO_FILE):
            os.remove(TEMP_AUDIO_FILE)

if __name__ == "__main__":
    main()